# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19KUIBf--9K0iq9JI4BBeR_bNjTvYz4zk

This program have some functions used by Jason Brownlee to scale and get the dataset
"""

import math
import numpy
import matplotlib.pyplot as plt
from csv import reader
from random import random

"""This program have some functions used by Jason Brownlee to scale and get the dataset"""

"""The next block of code is the one that is in charge of cleaning, building, spliting and scaling the db"""

def load_csv(filename):
  """ This function cleans the dataset from the rows that have Null and retur a clean dataset"""
  dataset = []
  with open(filename, 'r') as file:
    csvReader =  reader(file)
    for row in csvReader:
      if (not row) or ("NA" in row):
        continue
      dataset.append(row)
  dataset.pop(0)#This is to skip the names of the columns
  return dataset


def str_column_to_float(dataset):#This changes all the values to float values, to avoid problems with max function
  for i in range(len(dataset[0])):
    for row in dataset:
      row[i] = float(row[i].strip())

def scale(dataset): #this gives us values between -1 and 1
  minmax = []
  for i in range(len(dataset[0])): #This is looking in each column, row example:['0', '39', '4', '1', '9', '0', '0', '0', '0', '170', '110.5', '69', '22.19', '60', '103', '0']
    columnValues = [row[i] for row in dataset]
    acum = 0
    for row in columnValues:
      acum += row
    value_min = min(columnValues)
    value_max = max(columnValues)
    minmax.append([value_min, value_max])
    '''
    avg = acum / len(columnValues)
    std = numpy.std(columnValues)
    minmax.append([avg,std])'''
  for row in dataset:
    for i in range(len(row)):
      #row[i] = (row[i] - minmax[i][0]) / minmax[i][1]
      if(minmax[i][0] != 1):
         row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0]) 
      


def getYX(ds):
  y = []
  for row in ds:
    temp = row.pop(len(row)-1)
    if(temp>0):
      y.append(1)
    else:
      y.append(0)
  return y

def splitDs (dataset,splits):
  splitData = []
  cpyDs = list(dataset)
  splitSize = int(len(dataset)/splits)
  for i in range(splits):
    split= []
    for j in range(splitSize):
      split.append(cpyDs.pop())
    splitData.append(split)
  return splitData

def getDs(filename): #Build the complete and clean ds
  ds = load_csv(filename)
  str_column_to_float(ds)
  scale(ds)
  return splitDs(ds,2)

"""The next block is the one with the full algorithm of the Loggistic regression"""

def hy(weights, x):
  """ This function makes the hypothesis for the Logistic regressios
      weights: are the parameters that multiplies their respective x
      x: are the values from one sample
  """ 
  hyp = 0
  for i in range(len(weights)):
    hyp+=(weights[i]*x[i])
  hyp = hyp * (-1)
  hyp = 1/(1+ math.exp (hyp))
  return hyp

def gD(weights,x,y,a):
  """ Gradient descend function """
  temp=list(weights)
  for j in range(len(weights)):
      acum=0
      for i in range(len(x)):
          err=hy(weights,x[i])-y[i]
          acum+=err*x[i][j]
      temp[j]=weights[j]-a*(1/len(x))*acum
  return temp

"""The next block helps in monitoring the error of the model and its changes"""

def error(weights,x,y):
  acumError = 0
  avgError = 0;
  for i in range(len(x)):
    hyp = hy(weights,x[i])
    if(y[i] == 1): # avoid the log(0) error
      if(hyp ==0):
        hyp = .0001
      error = (-1)*math.log(hyp)
    if(y[i] == 0):
      if(hyp ==1):
        hyp = .9999
      error = (-1)*math.log(1-hyp)
    acumError += error
  #print( "error %f  hyp  %f  y %f " % (error, hyp,  y[i]))
  avgError = acumError/ len(x)
  return avgError

"""The next block its the function for the test part"""

def test(weights,x,y):
  print("\n \n Start test")
  acumLogError = 0
  acumError = 0
  for i in range(len(x)):
    hyp = hy(weights,x[i])
    if(y[i] == 1.0): # avoid the log(0) error
      if(hyp ==0):
        hyp = .0001
      logError = (-1)*math.log(hyp)
      error = 1-hyp
    if(y[i] == 0.0):
      if(hyp ==1):
        hyp = .9999
      logError = (-1)*math.log(1-hyp)
      error = hyp
    acumLogError += logError
    acumError += error
    print( "logError %f error %f  hyp  %f  y %f " % (logError,error, hyp,  y[i]))
  avgLogError = acumLogError/len(x)
  avgError = acumError/len(x)
  print(avgError)
  print(avgLogError)

"""And the last block its the one that calls the functions, its the main function"""

a = .3
weights =[]
for i in range(7):
  weights.append(random())
ds = getDs("framingham.csv")
dsTrainY = getYX(ds[0])
dsTrainX = ds[0]
dsTestY = getYX(ds[1])
dsTestX = ds[1]
epoch = 1000
err = 1
graph = []

while err > .15:
  temp = list(weights)
  weights = gD(weights, dsTrainX, dsTrainY,a)
  err = error(weights,dsTrainX,dsTrainY)
  graph.append(err)
  print(err)
  print(weights)

print("Weights:")
print(weights)
plt.plot(graph)

test(weights,dsTestX,dsTestY)

#from google.colab import drive
#drive.mount('/content/drive')

#[0.29090586562218795, 0.028404369449843366, 0.00901239127353041, 0.6455687364271405, 0.5447468140811955, -0.002632271655336762, 0.007932232301907524, -0.08836664243470042, -0.025540369582936717, 0.0020261383466354684]
#[-5.273028147447978, 0.7543417958793833, 2.659396314953786, 1.0833549286705457, 0.18801482140925246, 0.03065785306534585, 1.3202007218140084, 3.559455779114995, -0.11395834286014755, 0.45683864540369934, 1.9951409148347345]
